{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End MLOps Pipeline for Predicting Student Academic Risk"
      ],
      "metadata": {
        "id": "mqWTS7i83dfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project demonstrates a comprehensive Machine Learning Operations (MLOps) pipeline designed to predict the academic risk of students in higher education. The pipeline encompasses data preprocessing, model training, hyperparameter tuning, model evaluation, visualization, continuous integration, and deployment. By leveraging MLOps practices, the project ensures reproducibility, version control, and efficient deployment of machine learning models."
      ],
      "metadata": {
        "id": "pNOvrgmf4ZDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setup and Install Required Libraries\n",
        "First, we need to install the necessary libraries and set up the environment."
      ],
      "metadata": {
        "id": "4Rjr0mUt3kSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn mlflow joblib fastapi uvicorn pydantic docker"
      ],
      "metadata": {
        "id": "zno8jKWq3HVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Import Libraries\n",
        "Import the necessary libraries for data processing, modeling, and deployment."
      ],
      "metadata": {
        "id": "so9oEoKb3qq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import joblib\n",
        "import mlflow\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "from typing import List\n",
        "import logging"
      ],
      "metadata": {
        "id": "xaJwj1lo3HmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load and Explore the Dataset\n",
        "Load the dataset and perform initial exploration."
      ],
      "metadata": {
        "id": "xro_WC2C3ueF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Replace 'your_dataset.csv' with the actual path to your dataset\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(df.info())\n",
        "\n",
        "# Summary statistics of the dataset\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "ZsoPN4OY3HpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Data Preprocessing\n",
        "Perform data preprocessing steps including handling missing values, feature selection, encoding, and scaling."
      ],
      "metadata": {
        "id": "oXX1sj003yE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target variable\n",
        "TARGET = 'target_column_name'\n",
        "\n",
        "# Handle missing values (if any)\n",
        "# df = df.dropna()  # Example: Drop rows with missing values\n",
        "\n",
        "# Feature selection\n",
        "X = df.drop(columns=[TARGET, 'id'])  # Assuming 'id' is a column to be dropped\n",
        "y = df[TARGET]\n",
        "\n",
        "# Feature encoding\n",
        "categorical_features = ['Course']  # Example categorical feature\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# One-Hot Encoding for categorical features\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Label Encoding for the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', scaler, numerical_features),\n",
        "        ('cat', one_hot_encoder, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "X_preprocessed = preprocessor.fit_transform(X)"
      ],
      "metadata": {
        "id": "wLxRNZoX3HsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Model Selection and Training\n",
        "Train multiple models and evaluate their performance."
      ],
      "metadata": {
        "id": "ILRk2hnX31Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a list of models to evaluate\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(),\n",
        "    'GradientBoosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f'{model_name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')"
      ],
      "metadata": {
        "id": "vIzBk4yl3HvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Hyperparameter Tuning\n",
        "Perform hyperparameter tuning using RandomizedSearchCV."
      ],
      "metadata": {
        "id": "BO7Tgrev34un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Random Forest\n",
        "param_dist_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Define hyperparameters for Gradient Boosting\n",
        "param_dist_gb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for Random Forest\n",
        "random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist_rf, n_iter=10, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=2, random_state=42)\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Perform RandomizedSearchCV for Gradient Boosting\n",
        "random_search_gb = RandomizedSearchCV(GradientBoostingClassifier(), param_distributions=param_dist_gb, n_iter=10, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=2, random_state=42)\n",
        "random_search_gb.fit(X_train, y_train)\n",
        "\n",
        "# Get the best models\n",
        "best_rf = random_search_rf.best_estimator_\n",
        "best_gb = random_search_gb.best_estimator_\n",
        "\n",
        "# Evaluate the best models\n",
        "for model_name, model in [('RandomForest', best_rf), ('GradientBoosting', best_gb)]:\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f'{model_name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')"
      ],
      "metadata": {
        "id": "ZWoA48yH3Hyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Model Evaluation\n",
        "Evaluate the best model and generate predictions for the test set."
      ],
      "metadata": {
        "id": "6nPyYvfB37ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model (example: Random Forest)\n",
        "best_model = best_rf\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Best Model - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')"
      ],
      "metadata": {
        "id": "ukdPyoRa3H1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Visualization and Results Analysis\n",
        "Visualize the confusion matrix and feature importance."
      ],
      "metadata": {
        "id": "TwdRAGZu3-sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance (for Random Forest)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    importances = best_model.feature_importances_\n",
        "    feature_names = np.array(numerical_features.tolist() + list(one_hot_encoder.get_feature_names_out(categorical_features)))\n",
        "    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
        "    plt.title('Top 20 Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nKCvAyi53H4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Continuous Integration with MLflow\n",
        "Set up MLflow for experiment tracking."
      ],
      "metadata": {
        "id": "ZsFbxB2I4Bl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up MLflow\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_experiment(\"Student Academic Risk Prediction\")\n",
        "\n",
        "# Log the best model\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params(best_model.get_params())\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "    mlflow.log_metric(\"precision\", precision)\n",
        "    mlflow.log_metric(\"recall\", recall)\n",
        "    mlflow.sklearn.log_model(best_model, \"best_model\")"
      ],
      "metadata": {
        "id": "ijPzOYzu3H9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Model Deployment with FastAPI\n",
        "Deploy the model using FastAPI."
      ],
      "metadata": {
        "id": "KFM5NcEh4FLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define the input data model\n",
        "class PredictionInput(BaseModel):\n",
        "    features: List[float]\n",
        "\n",
        "# Define the prediction endpoint\n",
        "@app.post('/predict')\n",
        "def predict(input_data: PredictionInput):\n",
        "    try:\n",
        "        input_array = np.array(input_data.features).reshape(1, -1)\n",
        "        input_array = preprocessor.transform(input_array)\n",
        "        prediction = best_model.predict(input_array)\n",
        "        return {\"prediction\": int(prediction[0])}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Prediction error: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n",
        "\n",
        "# Run the FastAPI app\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "bgZkXAAB3IBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Dockerization\n",
        "Create a Dockerfile to containerize the FastAPI application."
      ],
      "metadata": {
        "id": "nuArmako4Ikl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY app.py /app/\n",
        "COPY requirements.txt /app/\n",
        "COPY models /app/models\n",
        "COPY static /app/static\n",
        "\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "EXPOSE 8000\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
      ],
      "metadata": {
        "id": "vMGRhLe73IEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12: Build and Run Docker Container\n",
        "Build and run the Docker container."
      ],
      "metadata": {
        "id": "z6wVqZbw4Mgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!docker build -t academic-risk-predictor .\n",
        "!docker run -p 8000:8000 academic-risk-predictor"
      ],
      "metadata": {
        "id": "HIWUYXzB3Z3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}